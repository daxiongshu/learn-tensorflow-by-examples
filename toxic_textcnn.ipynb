{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
      "       'insult', 'identity_hate'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "train.iloc[0,1]\n",
    "print(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic = train[train.toxic == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 8) (15294, 8)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, toxic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0005c987bdfc9d4b</td>\n",
       "      <td>Hey... what is it..\\n@ | talk .\\nWhat is it......</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0007e25b2121310b</td>\n",
       "      <td>Bye! \\n\\nDon't look, come or think of comming ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>001810bf8c45bf5f</td>\n",
       "      <td>You are gay or antisemmitian? \\n\\nArchangel WH...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>00190820581d90ce</td>\n",
       "      <td>FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                       comment_text  \\\n",
       "6   0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK   \n",
       "12  0005c987bdfc9d4b  Hey... what is it..\\n@ | talk .\\nWhat is it......   \n",
       "16  0007e25b2121310b  Bye! \\n\\nDon't look, come or think of comming ...   \n",
       "42  001810bf8c45bf5f  You are gay or antisemmitian? \\n\\nArchangel WH...   \n",
       "43  00190820581d90ce           FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!   \n",
       "\n",
       "    toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "6       1             1        1       0       1              0  \n",
       "12      1             0        0       0       0              0  \n",
       "16      1             0        0       0       0              0  \n",
       "42      1             0        1       0       1              1  \n",
       "43      1             0        1       0       1              0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Bye! \\n\\nDon't look, come or think of comming back! Tosser.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic.iloc[2,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are gay or antisemmitian? \\n\\nArchangel WHite Tiger\\n\\nMeow! Greetingshhh!\\n\\nUh, there are two ways, why you do erased my comment about WW2, that holocaust was brutally slaying of Jews and not gays/Gypsys/Slavs/anyone...\\n\\n1 - If you are anti-semitian, than shave your head bald and go to the skinhead meetings!\\n\\n2 - If you doubt words of the Bible, that homosexuality is a deadly sin, make a pentagram tatoo on your forehead go to the satanistic masses with your gay pals!\\n\\n3 - First and last warning, you fucking gay - I won't appreciate if any more nazi shwain would write in my page! I don't wish to talk to you anymore!\\n\\nBeware of the Dark Side!\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic.iloc[3,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class textCNN:\n",
    "    \n",
    "    def __init__(self,**params):\n",
    "        # params is a dictionary\n",
    "        self.params = params\n",
    "        self.y = None\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        # X is a list of comments. [[fxxk, ..], [something, ..]]\n",
    "        # len(X) == N == len(y)\n",
    "        # y is a numpy array (N, 6)\n",
    "        self.X = np.array(X)\n",
    "        self.y = y\n",
    "        self._train()\n",
    "    \n",
    "    def predict(self,X):\n",
    "        print('predict')\n",
    "        # X is a list of comments. [[fxxk, ..], [something, ..]]\n",
    "        self.X = X # Xt means X for test data\n",
    "        return self.predictx()\n",
    "    \n",
    "    def predictx(self):\n",
    "        tf.reset_default_graph()\n",
    "        self.params['epochs'] = 1\n",
    "        # build a tf computing graph\n",
    "        logit = self._build()\n",
    "        logit = tf.nn.sigmoid(logit)\n",
    "        preds = []\n",
    "        print('here')\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            sess.run(tf.local_variables_initializer())\n",
    "            self.load(sess)\n",
    "            for c,Xb in enumerate(self._batch_gen(shuffle=False)):\n",
    "                pred = sess.run(logit,feed_dict={self.inputs:Xb})\n",
    "                if c%10 == 0:\n",
    "                    print(\"batch %d predicted\"%(c))\n",
    "                preds.append(pred)\n",
    "        preds = np.vstack(preds)\n",
    "        return preds\n",
    "        \n",
    "    def _train(self):\n",
    "        tf.reset_default_graph()\n",
    "        # build a tf computing graph\n",
    "        logit = self._build()\n",
    "        label = tf.placeholder(dtype=tf.int32,shape=[None,6]) # B,classes\n",
    "        losst = self.get_loss(logit,label)\n",
    "        opt_op = self.get_opt(losst)\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            sess.run(tf.local_variables_initializer())\n",
    "            self.load(sess)\n",
    "            for c,(Xb,yb) in enumerate(self._batch_gen(shuffle=True)):\n",
    "                loss,_ = sess.run([losst,opt_op],feed_dict={self.inputs:Xb, label:yb})\n",
    "                if c%10 == 0:\n",
    "                    print(\"batch %d train loss %.4f\"%(c,loss))\n",
    "                if c>100:\n",
    "                    break\n",
    "            self.save(sess)\n",
    "        \n",
    "    def get_opt(self,loss):\n",
    "        opt = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "        return opt.minimize(loss)\n",
    "        \n",
    "    def get_loss(self, logit, label):\n",
    "        # build the loss tensor\n",
    "        label = tf.cast(label,tf.float32)\n",
    "        loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,labels=label)\n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "    def save(self, sess):\n",
    "        varss = tf.trainable_variables()\n",
    "        weights = {} # var.name => var.value: a numpy array\n",
    "        for var in varss:\n",
    "            val = sess.run(var)\n",
    "            weights[var.name] = val\n",
    "        pickle.dump(weights,open('weight.p','wb'))\n",
    "\n",
    "    def load(self, sess, path = 'weight.p'):\n",
    "        weights = pickle.load(open(path,'rb'))\n",
    "        varss = tf.trainable_variables()\n",
    "        for var in varss:\n",
    "            value = weights[var.name]\n",
    "            assign_op = var.assign(value)\n",
    "            sess.run(assign_op)\n",
    "        \n",
    "    def _build(self):\n",
    "        \n",
    "        # B is the batch size\n",
    "        # S is the sequence length\n",
    "        # E is the embedding size: 16 by default\n",
    "        # V is the vocab size\n",
    "        # embedding space will be: V x E\n",
    "        # self.inputs => Xb\n",
    "        E = self.params.get('embedding_size',16)\n",
    "        V = self.params['vocab_size']\n",
    "        \n",
    "        netname = 'textCNN'\n",
    "        self.inputs = tf.placeholder(dtype=tf.int32,shape=[None,None]) # B,S\n",
    "        \n",
    "        with tf.variable_scope(netname):\n",
    "            # embedding lookup\n",
    "            print(\"Input:\",self.inputs.get_shape().as_list())\n",
    "            net = self.embedding_lookup(E, V, self.inputs,reuse=False) # dim(net) = B,S,E\n",
    "            print(\"after embedding lookup:\",net.get_shape().as_list())\n",
    "            \n",
    "            net1 = self.conv1d(net,name='conv1',ngrams=3,stride=1,cin=E,nfilters=30)\n",
    "            net2 = self.conv1d(net,name='conv2',ngrams=1,stride=1,cin=E,nfilters=10)\n",
    "            net3 = self.conv1d(net,name='conv3',ngrams=5,stride=1,cin=E,nfilters=50)\n",
    "            \n",
    "            print(\"before maxpool net1: {} net2: {} net3: {}\".format(net1.get_shape().as_list(),net2.get_shape().as_list(),net3.get_shape().as_list()))\n",
    "            \n",
    "            net1 = tf.reduce_max(net1,axis=1)\n",
    "            net2 = tf.reduce_max(net2,axis=1)\n",
    "            net3 = tf.reduce_max(net3,axis=1)\n",
    "            \n",
    "            print(\"after maxpool net1: {} net2: {} net3: {}\".format(net1.get_shape().as_list(),net2.get_shape().as_list(),net3.get_shape().as_list()))\n",
    "           \n",
    "            \n",
    "            net  = tf.concat([net1,net2,net3],axis=1)\n",
    "            print(\"after concat:\",net.get_shape().as_list())\n",
    "            #print(\"after conv1d:\",net.get_shape().as_list())\n",
    "            \n",
    "            net = self.fc(net, 'fc', cin=90, cout=6, activation=None)\n",
    "            print(\"after fc:\",net.get_shape().as_list())\n",
    "            return net\n",
    "            \n",
    "    def fc(self, net, name, cin, cout, activation='relu'):\n",
    "        weights = tf.get_variable(name=name,shape=[cin,cout],dtype=tf.float32,\n",
    "            initializer=tf.contrib.layers.xavier_initializer())\n",
    "        net = tf.matmul(net,weights)\n",
    "        if activation == 'relu':\n",
    "            net = tf.nn.relu(net)\n",
    "        return net\n",
    "        \n",
    "    def conv1d(self, net, name, ngrams, stride, cin, nfilters,activation='relu'):\n",
    "        # filters: kernel_size, cin, cout\n",
    "        filters = tf.get_variable(name='%s_filters'%name,shape=[ngrams,cin,nfilters],dtype=tf.float32,\n",
    "            initializer=tf.contrib.layers.xavier_initializer()) # filter variable\n",
    "        net = tf.nn.conv1d(net, filters, stride, padding='VALID')\n",
    "        if activation == 'relu':\n",
    "            net = tf.nn.relu(net)\n",
    "        return net\n",
    "    \n",
    "    def embedding_lookup(self,E,V,x,reuse=False):\n",
    "        # x is the key\n",
    "        with tf.variable_scope('embedding', reuse=reuse):\n",
    "            \n",
    "            # initialize the embedding matrix variable\n",
    "            w = tf.get_variable(name='w',shape=[V-1,E],dtype=tf.float32,initializer=tf.contrib.layers.xavier_initializer()) # embedding matrix\n",
    "            # w.name = 'textCNN/embedding/w:0'\n",
    "            self.em = w\n",
    "            c = tf.zeros([1,E])\n",
    "            w = tf.concat([c,w],axis=0)\n",
    "            \n",
    "            \n",
    "            # embedding lookup\n",
    "            x = tf.cast(x,tf.int32)\n",
    "            x = tf.nn.embedding_lookup(w, x, name='word_vector')  # (N, T, M) or (N, M)\n",
    "            return x\n",
    "    \n",
    "    def _batch_gen(self, shuffle=True):\n",
    "        X,y = self.X, self.y\n",
    "        B = self.params.get('batch_size', 128)\n",
    "        epochs = self.params.get('epochs', 10)\n",
    "        ids = [i for i in range(len(X))]\n",
    "        batches = len(X)//B + 1\n",
    "        print(epochs,batches)\n",
    "        for epoch in range(epochs):\n",
    "            if shuffle:\n",
    "                random.shuffle(ids)\n",
    "            for i in range(batches):                \n",
    "                idx = ids[i*B:(i+1)*B]\n",
    "                Xb = self.padding(X[idx])\n",
    "                if y is not None:\n",
    "                    yield Xb,y[idx]\n",
    "                else:\n",
    "                    yield Xb\n",
    "            if (i+1)*B < len(X):\n",
    "                idx = ids[(i+1)*B:len(X)]\n",
    "                Xb = self.padding(X[idx])\n",
    "                yield Xb\n",
    "    \n",
    "    def padding(self,X):\n",
    "        Xn = []\n",
    "        maxlen = max([len(i) for i in X])\n",
    "        for x in X:\n",
    "            xn = x+[0]*(maxlen-len(x))\n",
    "            assert len(xn)==maxlen\n",
    "            Xn.append(xn)\n",
    "        return np.array(Xn)\n",
    "    \n",
    "    def _predict(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 rows done, size of vocab = 237044\n",
      "100000 rows done, size of vocab = 384696\n",
      "150000 rows done, size of vocab = 509860\n"
     ]
    }
   ],
   "source": [
    "# construct a vocabulary as a dictionary: {word: integer}\n",
    "vocab = {'__none__':0}\n",
    "for index, row in train.iterrows():\n",
    "    comment = row['comment_text']\n",
    "    if index >0 and index%50000 ==0:\n",
    "        print('%d rows done, size of vocab = %d'%(index,len(vocab)))\n",
    "    words = comment.split()\n",
    "    for word in words:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 rows done\n",
      "100000 rows done\n",
      "150000 rows done\n"
     ]
    }
   ],
   "source": [
    "# map the word to an integer for all the words\n",
    "# build the X\n",
    "X = []\n",
    "for index, row in train.iterrows():\n",
    "    comment = row['comment_text']\n",
    "    if index >0 and index%50000 ==0:\n",
    "        print('%d rows done'%(index))\n",
    "    words = comment.split()\n",
    "    x = []\n",
    "    for word in words:\n",
    "        x.append(vocab.get(word, 0))\n",
    "    X.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 rows done\n",
      "100000 rows done\n",
      "150000 rows done\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('../input/test.csv.zip')\n",
    "Xt = []\n",
    "for index, row in test.iterrows():\n",
    "    comment = row['comment_text']\n",
    "    if index >0 and index%50000 ==0:\n",
    "        print('%d rows done'%(index))\n",
    "    words = comment.split()\n",
    "    x = []\n",
    "    for word in words:\n",
    "        x.append(vocab.get(word, 0))\n",
    "    Xt.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = train.columns.values[2:]\n",
    "Xt = np.array(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train[classes].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159571 159571 (159571, 6)\n"
     ]
    }
   ],
   "source": [
    "print(len(X),len(y),y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = textCNN(vocab_size=len(vocab),embedding_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [None, None]\n",
      "after embedding lookup: [None, None, 4]\n",
      "before maxpool net1: [None, None, 30] net2: [None, None, 10] net3: [None, None, 50]\n",
      "after maxpool net1: [None, 30] net2: [None, 10] net3: [None, 50]\n",
      "after concat: [None, 90]\n",
      "after fc: [None, 6]\n",
      "batch 0 train loss 0.6932\n",
      "batch 10 train loss 0.6878\n",
      "batch 20 train loss 0.6757\n",
      "batch 30 train loss 0.6538\n",
      "batch 40 train loss 0.6171\n",
      "batch 50 train loss 0.5593\n",
      "batch 60 train loss 0.4951\n",
      "batch 70 train loss 0.4168\n",
      "batch 80 train loss 0.3404\n",
      "batch 90 train loss 0.2365\n",
      "batch 100 train loss 0.2042\n"
     ]
    }
   ],
   "source": [
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [None, None]\n",
      "after embedding lookup: [None, None, 4]\n",
      "before maxpool net1: [None, None, 30] net2: [None, None, 10] net3: [None, None, 50]\n",
      "after maxpool net1: [None, 30] net2: [None, 10] net3: [None, 50]\n",
      "after concat: [None, 90]\n",
      "after fc: [None, 6]\n",
      "batch 0 train loss 0.1849\n",
      "batch 10 train loss 0.1930\n",
      "batch 20 train loss 0.1750\n",
      "batch 30 train loss 0.1424\n",
      "batch 40 train loss 0.1465\n",
      "batch 50 train loss 0.1788\n",
      "batch 60 train loss 0.1234\n",
      "batch 70 train loss 0.1420\n",
      "batch 80 train loss 0.1780\n",
      "batch 90 train loss 0.1153\n",
      "batch 100 train loss 0.1293\n"
     ]
    }
   ],
   "source": [
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = textCNN(vocab_size=len(vocab),embedding_size=4)\n",
    "yp = model.predict(Xt)\n",
    "yp.shape\n",
    "s = pd.DataFrame(yp,columns = ['toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate'])\n",
    "s['id'] = test['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = pickle.load(open('weight.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "textCNN/conv1_filters:0 (3, 4, 30)\n",
      "textCNN/conv2_filters:0 (1, 4, 10)\n",
      "textCNN/embedding/w:0 (532299, 4)\n",
      "textCNN/conv3_filters:0 (5, 4, 50)\n",
      "textCNN/fc:0 (90, 6)\n"
     ]
    }
   ],
   "source": [
    "for name,values in weights.items():\n",
    "    print(name,values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "varss = tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(varss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
